{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Problem\n",
    "\n",
    "This should be fun. Try to use sklearn to implement SkipGrams using the sklearn.neural_network.MLPClassifier class.\n",
    "\n",
    "#### Remember:\n",
    "\n",
    "The input to SkipGrams is a one-hot encoded vector for the word under examination in a context window.  \n",
    "The output is the similar one-hot vectors for the remaining words in the window  \n",
    "\n",
    "#### Effectively:\n",
    "You can treat the outputs as if it were a multiclass, multilabel problem. That is, each training observation (a context window) will have a 1-hot encoded vector as input (representing the middle word in the window) and 4 integers as output (representing the \"classes\" for that observation, with the classes here being the indexes of the context words in your vocabulary).  \n",
    "With this formulation, you should be able to make it work with the MLPClassifier, although start with very small amounts of data.  \n",
    "\n",
    "#### Data:\n",
    "Use the 20 newsgroups data from sklearn but only take a small sample of the data. word2vec is not quick.\n",
    "\n",
    "#### Suggested Steps:\n",
    "Set a Window size (the number of words before and after the target word included in the context window, I suggest 2 for now)\n",
    "Use a CountVectorizer to establish the term indexes in your vocabulary\n",
    "For each context window, use the CountVectorizer to get the appropriate 1-hot vector as input and 4 integers (labels) as output (representing the 4 context words for a window size of 2)\n",
    "Plug in your observations (X=1-hot vector, and y=vector of 4 integers) and fit an MLPClassifier and see how it does!\n",
    "\n",
    "That is, you need to use the coef_ (the weights matrix W) attribute from your CountVectorizer to map the word vectors into the new word2vec space. The transformation for this remember, is W'x where W' is the transpose of W and x is the 1-hot vector for a word.\n",
    "Compute some cosine similarities between a few terms using the new W'x vectors. Any luck at all?\n",
    "Feel free to collaborate extensively on this one. I don't want it to take too too long but I understand it is involved.\n",
    "\n",
    "May the best vectors win!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset='train', \n",
    "    shuffle=True, \n",
    "    random_state=42, \n",
    "    categories=['sci.space'],\n",
    "    remove=('header', 'footer')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    tokenizer=word_tokenize,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=stopwords.words('english'),\n",
    ")\n",
    "vectors = vectorizer.fit_transform(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593, 100584)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
